
Extra Notes:

The anchor points in the .json file are used during the training and the inference.
Here it is pre-defined but they are first extracted using the k-means clustering.
K-means clustering on the bounding boxes we get before the training phase and 
the representive box is chosen 
as the anchor, based on some metric may be like the centroid.



activation and weight quantization specific params in the .alls files:
---------------------------------------------------------------------------------------------
quantization_param(output_layer3, precision_mode=a16_w16)
quantization_param(output_layer6, precision_mode=a16_w16)
quantization_param(output_layer9, precision_mode=a16_w16)

why ? for the particular layers the precision mode is 16 , why not for all the layers ?

co pilot response :
Why Specific Layers Use a16_w16
Sensitivity to Precision: Some layers, especially those closer to the output, 
are more sensitive to precision loss. Using higher precision (16-bit) helps maintain 
accuracy. Layers that perform complex operations or have a high impact on the final output
might require higher precision to avoid significant accuracy degradation.
Layer Functionality: Output layers or layers involved in critical computations (e.g., final convolutional layers, fully connected layers) might need higher precision to ensure the quality of the results.
Intermediate layers might tolerate lower precision without a noticeable impact on performance.
Empirical Optimization: The choice of precision is often based on empirical results. During model optimization, different precision settings are tested, and the ones that provide the best trade-off between performance and accuracy are chosen.
The specific layers mentioned (output_layer3, output_layer6, output_layer9) might have been identified as requiring higher precision through such empirical testing.






Logs while optimising the yolov8m with the modified config files.
---------------------------------------------------------------------------------------------
The changes are copied from the yolov5m config files
which are straight away working if use the hailomz cli cmds. When said working, means the inference at the hailo device 
from tge .hef file generated.

└─$ hailomz optimize --har ~/Desktop/yolov8m_mod_trial_with_mod_config_files_from_yolov5m/yolov8m_parse.har  yolov8m
<Hailo Model Zoo INFO> Start run for network yolov8m ...
<Hailo Model Zoo INFO> Initializing the hailo8 runner...
<Hailo Model Zoo INFO> Preparing calibration data...
[info] Loading model script commands to yolov8m from /home/rubesh/Desktop/dfc_3_27/hailo_model_zoo/hailo_model_zoo/cfg/alls/generic/yolov8m.alls
[info] The layer yolov8m/conv57 was detected as reg_layer.
[info] The layer yolov8m/conv70 was detected as reg_layer.
[info] The layer yolov8m/conv82 was detected as reg_layer.
[info] Starting Model Optimization
[warning] Reducing optimization level to 0 (the accuracy won't be optimized and compression won't be used) because there's no available GPU
[info] Model received quantization params from the hn
[info] Starting Mixed Precision
[info] Mixed Precision is done (completion time is 00:00:00.07)
[info] create_layer_norm skipped
[info] Starting Stats Collector
[info] Using dataset with 64 entries for calibration


what is the distill loss per layer shown here in the logs :

- _distill_loss_yolov8m/conv82: 0.0968 
- _distill_loss_yolov8m/conv83: 0.2189
- _distill_loss_yolov8m/conv83: 0.2183 
- _distill_loss_yolov8m/conv77: 0.2344

Distillation loss is a crucial component of the model optimization process in Hailo devices.
It helps the student model learn from the teacher model, improving its performance and accuracy. 

Logs :

Each _distill_loss_yolov8m/convXX entry represents the distillation loss for a specific layer.
The total_distill_loss is the sum of all these individual layer losses, 
providing an overall measure of how well the student model is learning 
from the teacher model.

399/512 [======================>.......] - ETA: 11:04 
- total_distill_loss: 1.6257 
- _distill_loss_yolov8m/conv57: 0.0982 
- _distill_loss_yolov8m/conv58: 0.2753 
- _distill_loss_yolov8m/conv70: 0.1062 
- _distill_loss_yolov8m/conv71: 0.2151 
- _distill_loss_yolov8m/conv82: 0.0958 
- _distill_loss_yolov8m/conv83: 0.2077 
- _distill_loss_yolov8m/conv77: 0.2329 
- _distill_loss_yolov8m/conv64: 0.2445 
- _distill_loss_y400/512 [======================>.......] - ETA: 10:58 
- total_distill_loss: 1.6245 
- _distill_loss_yolov8m/conv57: 0.0982 
- _distill_loss_yolov8m/conv58: 0.2750 
- _distill_loss_yolov8m/conv70: 0.1061 
- _distill_loss_yolov8m/conv71: 0.2147 
- _distill_loss_yolov8m/conv82: 0.0957 
- _distill_loss_yolov8m/conv83: 0.2074 
- _distill_loss_yolov8m/conv77: 0.2329 
- _distill_loss_yolov8m/conv64: 0.2444 